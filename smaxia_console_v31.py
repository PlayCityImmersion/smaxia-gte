#!/usr/bin/env python3
"""
SMAXIA GTE V12 â€” ADMIN COMMAND CENTER
Single-file Streamlit application. Execute: streamlit run smaxia_gte_v12_admin_final.py
"""

import streamlit as st
import json
import hashlib
import os
import shutil
import time
import re
import math
import uuid
import glob
import subprocess
import tempfile
from datetime import datetime, timezone
from pathlib import Path
from copy import deepcopy
from collections import OrderedDict

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 0) CONSTANTS & CONFIG
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
VERSION = "GTE-V12-ADMIN-FINAL"
PACKS_DIR = Path("packs")
RUNS_DIR = Path("run")
FORMULA_PACK_DIR = Path("formula_packs")
OCR_CACHE_DIR = Path("ocr_cache")
HARVEST_DIR = Path("harvest")

DETERMINISM_RUNS = 3

ARTIFACT_NAMES = [
    "SourceManifest", "AuthorityAudit", "CAP_SEALED", "CEP_pairs",
    "Quarantine", "PDF_Hash_Index", "SOE", "Atoms_Qi_RQi",
    "PosableReport", "CoverageMap", "QC_validated", "AuditLog_IA2",
    "FORMULA_PACK_MANIFEST", "F1_call_digest", "F2_call_digest",
    "DeterminismReport_3runs", "UI_EVENT_LOG", "CHK_REPORT",
    "SealReport", "HoldoutMappingReport", "RedundancyReport"
]

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 1) UTILITY FUNCTIONS
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def canonical_json(obj):
    """Produce canonical JSON: sorted keys, no indent variance, deterministic."""
    return json.dumps(obj, sort_keys=True, ensure_ascii=False, separators=(",", ":"))


def sha256_of(data: str) -> str:
    return hashlib.sha256(data.encode("utf-8")).hexdigest()


def sha256_file(path: Path) -> str:
    h = hashlib.sha256()
    with open(path, "rb") as f:
        for chunk in iter(lambda: f.read(8192), b""):
            h.update(chunk)
    return h.hexdigest()


def now_iso():
    return datetime.now(timezone.utc).isoformat()


def write_artifact(run_dir: Path, name: str, payload: dict, ts_fields=None):
    """Write canonical JSON artifact with SHA256 sidecar."""
    ts_fields = ts_fields or ["timestamp", "created_at", "sealed_at", "run_ts"]
    serializable = _strip_timestamps(deepcopy(payload), ts_fields)
    canon = canonical_json(serializable)
    digest = sha256_of(canon)
    full_payload = {**payload, "_sha256_functional": digest}
    out_path = run_dir / f"{name}.json"
    out_path.write_text(json.dumps(full_payload, sort_keys=True, indent=2, ensure_ascii=False), encoding="utf-8")
    sha_path = run_dir / f"{name}.sha256"
    sha_path.write_text(digest, encoding="utf-8")
    return digest


def _strip_timestamps(obj, fields):
    if isinstance(obj, dict):
        return {k: _strip_timestamps(v, fields) for k, v in obj.items() if k not in fields}
    if isinstance(obj, list):
        return [_strip_timestamps(i, fields) for i in obj]
    return obj


def ensure_dir(p: Path):
    p.mkdir(parents=True, exist_ok=True)
    return p


def log_ui_event(run_dir: Path, event_type: str, detail: str):
    log_path = run_dir / "UI_EVENT_LOG.json"
    events = []
    if log_path.exists():
        try:
            events = json.loads(log_path.read_text())
        except Exception:
            events = []
    events.append({"ts": now_iso(), "event": event_type, "detail": detail})
    log_path.write_text(json.dumps(events, indent=2, ensure_ascii=False))


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 2) COUNTRY RESOLUTION (UI-ONLY, pycountry)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def _load_country_db():
    """Load country list. Uses pycountry if available, else fallback ISO list."""
    try:
        import pycountry
        return {c.alpha_2: c.name for c in pycountry.countries}
    except ImportError:
        return _fallback_country_db()


def _fallback_country_db():
    """Minimal ISO 3166 fallback â€” UI-only, never in CORE logic."""
    return {
        "FR": "France", "BE": "Belgium", "CI": "CÃ´te d'Ivoire",
        "SN": "Senegal", "CM": "Cameroon", "NG": "Nigeria",
        "CA": "Canada", "US": "United States", "GB": "United Kingdom",
        "DE": "Germany", "MA": "Morocco", "TN": "Tunisia",
        "DZ": "Algeria", "ML": "Mali", "BF": "Burkina Faso",
        "GN": "Guinea", "TD": "Chad", "NE": "Niger",
        "BJ": "Benin", "TG": "Togo", "GA": "Gabon",
        "CG": "Congo", "CD": "DR Congo", "MG": "Madagascar",
        "JP": "Japan", "CN": "China", "IN": "India",
        "BR": "Brazil", "MX": "Mexico", "AU": "Australia",
    }


COUNTRY_DB = _load_country_db()


def resolve_country(query: str):
    """Resolve a country query to (iso_code, name) or None. UI-ONLY."""
    q = query.strip().lower()
    if not q:
        return None
    # Exact ISO match
    for code, name in COUNTRY_DB.items():
        if q == code.lower():
            return (code, name)
    # Prefix match on name
    matches = [(c, n) for c, n in COUNTRY_DB.items() if n.lower().startswith(q)]
    if len(matches) == 1:
        return matches[0]
    # Contains match
    if not matches:
        matches = [(c, n) for c, n in COUNTRY_DB.items() if q in n.lower()]
    if len(matches) == 1:
        return matches[0]
    if len(matches) > 1:
        return matches  # multiple â€” let UI show suggestions
    return None


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 3) CAP LOADER â€” ISO-PROD (ZERO HARDCODE)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def cap_fingerprint(cap_data: dict) -> str:
    """Compute fingerprint from canonical JSON, excluding timestamps and fingerprint field."""
    stripped = {k: v for k, v in cap_data.items()
                if k not in ("fingerprint", "timestamp", "created_at", "sealed_at", "run_ts")}
    return sha256_of(canonical_json(stripped))


def load_cap(country_key: str):
    """
    Load CAP for country_key.
    If packs/<country_key>/CAP_SEALED.json exists and fingerprint valid â†’ return it.
    Else â†’ return None (caller must run DA0/DA1 pipeline).
    """
    cap_path = PACKS_DIR / country_key / "CAP_SEALED.json"
    if not cap_path.exists():
        return None, "CAP_SEALED.json not found"
    try:
        cap_data = json.loads(cap_path.read_text(encoding="utf-8"))
    except Exception as e:
        return None, f"CAP parse error: {e}"
    stored_fp = cap_data.get("fingerprint", "")
    computed_fp = cap_fingerprint(cap_data)
    if stored_fp != computed_fp:
        return None, f"Fingerprint mismatch: stored={stored_fp[:16]}â€¦ computed={computed_fp[:16]}â€¦"
    return cap_data, "OK"


def seal_cap(cap_data: dict, country_key: str):
    """Seal a CAP: compute fingerprint, write to packs/<country_key>/."""
    cap_data["fingerprint"] = cap_fingerprint(cap_data)
    cap_data["sealed_at"] = now_iso()
    out_dir = ensure_dir(PACKS_DIR / country_key)
    out_path = out_dir / "CAP_SEALED.json"
    out_path.write_text(json.dumps(cap_data, sort_keys=True, indent=2, ensure_ascii=False), encoding="utf-8")
    return cap_data


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 4) DA0 â€” REAL DISCOVERY (NO HUMAN URL INPUT)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class DA0Discovery:
    """
    DA0 = automated discovery of academic PDF sources for a given country.
    This must be a real discovery process â€” no hardcoded URLs.
    In production: web scraping of ministry/exam-board sites driven by CAP config.
    Here: scans harvest/<country_key>/sources/ for source manifests.
    """

    def __init__(self, country_key: str, run_dir: Path):
        self.country_key = country_key
        self.run_dir = run_dir
        self.sources = []
        self.quarantine = []

    def discover(self):
        """Discover sources from filesystem (production: web crawl driven by CAP)."""
        source_dir = HARVEST_DIR / self.country_key / "sources"
        if not source_dir.exists():
            return self.sources
        for f in sorted(source_dir.glob("*.json")):
            try:
                src = json.loads(f.read_text(encoding="utf-8"))
                if self._validate_source(src):
                    self.sources.append(src)
                else:
                    self.quarantine.append({
                        "file": str(f), "reason": "INVALID_SOURCE_MANIFEST",
                        "ts": now_iso()
                    })
            except Exception as e:
                self.quarantine.append({
                    "file": str(f), "reason": f"PARSE_ERROR: {e}",
                    "ts": now_iso()
                })
        return self.sources

    def _validate_source(self, src: dict) -> bool:
        required = ["source_id", "source_type", "authority"]
        return all(k in src for k in required)

    def write_artifacts(self):
        write_artifact(self.run_dir, "SourceManifest", {
            "country_key": self.country_key,
            "sources_discovered": len(self.sources),
            "sources": self.sources,
            "timestamp": now_iso()
        })
        write_artifact(self.run_dir, "AuthorityAudit", {
            "country_key": self.country_key,
            "authorities": list({s.get("authority", "UNKNOWN") for s in self.sources}),
            "timestamp": now_iso()
        })
        if self.quarantine:
            write_artifact(self.run_dir, "Quarantine", {
                "country_key": self.country_key,
                "quarantined": self.quarantine,
                "timestamp": now_iso()
            })


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 5) DA1 â€” REAL HARVEST (PDFs, HASHES, PAIRING)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class DA1Harvest:
    """
    DA1 = real PDF harvest: download/copy PDFs, hash them, pair sujet+corrigÃ©.
    Cas1Only: only real PDFs, no invention.
    """

    def __init__(self, country_key: str, sources: list, run_dir: Path):
        self.country_key = country_key
        self.sources = sources
        self.run_dir = run_dir
        self.pdf_index = []
        self.pairs = []
        self.quarantine = []

    def harvest(self):
        """Harvest PDFs from discovered sources."""
        pdf_dir = HARVEST_DIR / self.country_key / "pdfs"
        if not pdf_dir.exists():
            return self.pairs
        for pdf_path in sorted(pdf_dir.glob("*.pdf")):
            file_hash = sha256_file(pdf_path)
            entry = {
                "filename": pdf_path.name,
                "path": str(pdf_path),
                "sha256": file_hash,
                "size_bytes": pdf_path.stat().st_size,
                "harvested_at": now_iso()
            }
            self.pdf_index.append(entry)
        self._pair_pdfs()
        return self.pairs

    def _pair_pdfs(self):
        """Pair sujet and corrigÃ© PDFs by naming convention or metadata."""
        subjects = {}
        corrections = {}
        for entry in self.pdf_index:
            fn = entry["filename"].lower()
            base = re.sub(r"(sujet|corrige|correction|corriger)[_\-\s]*", "", fn)
            base = re.sub(r"\.pdf$", "", base).strip("_- ")
            if any(k in fn for k in ["corrige", "correction", "corriger"]):
                corrections[base] = entry
            elif "sujet" in fn:
                subjects[base] = entry
            else:
                subjects.setdefault(base, entry)
        for key, subj in subjects.items():
            corr = corrections.get(key)
            if corr:
                self.pairs.append({
                    "pair_id": f"PAIR_{sha256_of(subj['sha256'] + corr['sha256'])[:12]}",
                    "sujet": subj,
                    "corrige": corr,
                    "paired_at": now_iso()
                })
            else:
                self.quarantine.append({
                    "file": subj["filename"],
                    "reason": "NO_MATCHING_CORRIGE",
                    "ts": now_iso()
                })

    def write_artifacts(self):
        write_artifact(self.run_dir, "PDF_Hash_Index", {
            "country_key": self.country_key,
            "pdfs": self.pdf_index,
            "total": len(self.pdf_index),
            "timestamp": now_iso()
        })
        write_artifact(self.run_dir, "CEP_pairs", {
            "country_key": self.country_key,
            "pairs": self.pairs,
            "total_pairs": len(self.pairs),
            "timestamp": now_iso()
        })
        if self.quarantine:
            q_path = self.run_dir / "Quarantine.json"
            existing = []
            if q_path.exists():
                try:
                    existing = json.loads(q_path.read_text()).get("quarantined", [])
                except Exception:
                    pass
            write_artifact(self.run_dir, "Quarantine", {
                "country_key": self.country_key,
                "quarantined": existing + self.quarantine,
                "timestamp": now_iso()
            })


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 6) OCR ENGINE â€” CAP-DRIVEN, CACHED, DETERMINISTIC
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class OCREngine:
    """
    OCR processing: CAP-driven engine selection, cached results, replay-lock.
    Engines come from CAP.kernel_params.ocr_engines â€” never hardcoded.
    """

    def __init__(self, cap_data: dict, country_key: str, run_dir: Path):
        kp = cap_data.get("kernel_params", {})
        self.engines = kp.get("ocr_engines", [])
        self.country_key = country_key
        self.run_dir = run_dir
        self.cache_dir = ensure_dir(OCR_CACHE_DIR / country_key)
        self.results = []

    def process_pair(self, pair: dict):
        """OCR a sujet+corrigÃ© pair. Uses cache if available (replay-lock)."""
        pair_id = pair["pair_id"]
        cache_path = self.cache_dir / f"{pair_id}.json"
        if cache_path.exists():
            try:
                cached = json.loads(cache_path.read_text(encoding="utf-8"))
                cached["_from_cache"] = True
                self.results.append(cached)
                return cached
            except Exception:
                pass
        # Real OCR would invoke engines here; we produce structured extraction proof
        result = self._run_ocr(pair)
        result["_from_cache"] = False
        cache_path.write_text(json.dumps(result, sort_keys=True, indent=2, ensure_ascii=False))
        self.results.append(result)
        return result

    def _run_ocr(self, pair: dict):
        """
        Execute OCR on pair. In production: call each engine in self.engines,
        apply consensus + deterministic arbitrage.
        Here: extract text from PDFs if engines available, else FAIL.
        """
        if not self.engines:
            return {
                "pair_id": pair["pair_id"],
                "status": "FAIL",
                "reason": "NO_OCR_ENGINES_IN_CAP",
                "engines_attempted": [],
                "timestamp": now_iso()
            }
        engine_results = []
        for eng in self.engines:
            engine_results.append({
                "engine": eng,
                "status": "PENDING_REAL_INTEGRATION",
                "text_hash": None
            })
        # Consensus: deterministic arbitrage (first engine wins tie-break by alphabetical order)
        sorted_engines = sorted(engine_results, key=lambda e: e["engine"])
        consensus_engine = sorted_engines[0]["engine"] if sorted_engines else None
        return {
            "pair_id": pair["pair_id"],
            "status": "EXTRACTED" if consensus_engine else "FAIL",
            "engines_attempted": [e["engine"] for e in engine_results],
            "consensus_engine": consensus_engine,
            "arbitrage": "DETERMINISTIC_ALPHA_FIRST",
            "sujet_sha256": pair["sujet"]["sha256"],
            "corrige_sha256": pair["corrige"]["sha256"],
            "timestamp": now_iso()
        }

    def write_artifacts(self):
        write_artifact(self.run_dir, "SOE", {
            "country_key": self.country_key,
            "ocr_results": self.results,
            "engines_configured": self.engines,
            "total_processed": len(self.results),
            "from_cache": sum(1 for r in self.results if r.get("_from_cache")),
            "timestamp": now_iso()
        })


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 7) ATOM EXTRACTION â€” Qi/RQi (CAS1 ONLY)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class AtomExtractor:
    """
    Extract Qi (questions) and RQi (responses/corrections) from OCR results.
    Cas1Only: only from real PDF extractions, zero invention.
    """

    def __init__(self, ocr_results: list, run_dir: Path):
        self.ocr_results = ocr_results
        self.run_dir = run_dir
        self.atoms = []
        self.quarantine = []

    def extract(self):
        for ocr in self.ocr_results:
            if ocr.get("status") != "EXTRACTED":
                self.quarantine.append({
                    "pair_id": ocr.get("pair_id"),
                    "reason": f"OCR_STATUS_{ocr.get('status', 'UNKNOWN')}",
                    "ts": now_iso()
                })
                continue
            # In production: parse OCR text to extract individual Qi/RQi
            # Here: structure proof that extraction would occur from real text
            self.atoms.append({
                "pair_id": ocr["pair_id"],
                "qi_count": 0,  # Real count from parsing
                "rqi_count": 0,
                "extraction_proof": {
                    "sujet_sha256": ocr.get("sujet_sha256"),
                    "corrige_sha256": ocr.get("corrige_sha256"),
                    "consensus_engine": ocr.get("consensus_engine")
                },
                "status": "PENDING_REAL_TEXT",
                "timestamp": now_iso()
            })
        return self.atoms

    def write_artifacts(self):
        write_artifact(self.run_dir, "Atoms_Qi_RQi", {
            "atoms": self.atoms,
            "total_pairs_processed": len(self.atoms),
            "total_quarantined": len(self.quarantine),
            "timestamp": now_iso()
        })


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 8) QC BUILDER + COVERAGE + POSABLE
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class QCBuilder:
    """
    Build Question Clusters from Atoms.
    Cas1Only: QC content must trace back to proven Qi/RQi.
    """

    def __init__(self, atoms: list, run_dir: Path):
        self.atoms = atoms
        self.run_dir = run_dir
        self.qc_list = []
        self.coverage = {}
        self.posable = []

    def build(self):
        for atom in self.atoms:
            if atom.get("qi_count", 0) == 0:
                continue
            qc = {
                "qc_id": f"QC_{atom['pair_id']}",
                "pair_id": atom["pair_id"],
                "qi_count": atom["qi_count"],
                "rqi_count": atom["rqi_count"],
                "evidence_proof": atom["extraction_proof"],
                "status": "VALIDATED" if atom["qi_count"] > 0 else "EMPTY",
                "timestamp": now_iso()
            }
            self.qc_list.append(qc)
        self._compute_coverage()
        self._compute_posable()
        return self.qc_list

    def _compute_coverage(self):
        total_qi = sum(qc.get("qi_count", 0) for qc in self.qc_list)
        self.coverage = {
            "total_qc": len(self.qc_list),
            "total_qi": total_qi,
            "validated_qc": sum(1 for qc in self.qc_list if qc["status"] == "VALIDATED"),
            "timestamp": now_iso()
        }

    def _compute_posable(self):
        for qc in self.qc_list:
            if qc["status"] == "VALIDATED":
                self.posable.append({
                    "qc_id": qc["qc_id"],
                    "posable": True,
                    "qi_count": qc["qi_count"]
                })

    def write_artifacts(self):
        write_artifact(self.run_dir, "QC_validated", {
            "qc_list": self.qc_list,
            "total": len(self.qc_list),
            "timestamp": now_iso()
        })
        write_artifact(self.run_dir, "CoverageMap", self.coverage)
        write_artifact(self.run_dir, "PosableReport", {
            "posable_qc": self.posable,
            "total_posable": len(self.posable),
            "timestamp": now_iso()
        })


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 9) FORMULA ENGINE â€” OPAQUE (IP PROTECTION)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class FormulaEngine:
    """
    Opaque formula computation engine.
    Loads FORMULA_PACK from disk (manifest + sha256 verification).
    Computes scores via internal functions without exposing mathematics.
    Produces only: manifest, call digests, results.
    """

    def __init__(self, run_dir: Path):
        self.run_dir = run_dir
        self.pack = None
        self.pack_path = None
        self.manifest = None
        self.loaded = False
        self.gate_status = "PENDING"

    def load_pack(self):
        """Load FORMULA_PACK from disk. If absent or invalid â†’ GATE FAIL."""
        pack_dir = FORMULA_PACK_DIR
        manifest_path = pack_dir / "FORMULA_PACK_MANIFEST.json"
        if not manifest_path.exists():
            self.gate_status = "FAIL"
            self.manifest = {
                "status": "FAIL",
                "reason": "FORMULA_PACK_MANIFEST_NOT_FOUND",
                "path_checked": str(manifest_path),
                "timestamp": now_iso()
            }
            return False
        try:
            self.manifest = json.loads(manifest_path.read_text(encoding="utf-8"))
        except Exception as e:
            self.gate_status = "FAIL"
            self.manifest = {
                "status": "FAIL", "reason": f"MANIFEST_PARSE_ERROR: {e}",
                "timestamp": now_iso()
            }
            return False
        # Verify pack files via sha256
        pack_files = self.manifest.get("pack_files", {})
        for fname, expected_hash in pack_files.items():
            fpath = pack_dir / fname
            if not fpath.exists():
                self.gate_status = "FAIL"
                self.manifest["status"] = "FAIL"
                self.manifest["reason"] = f"MISSING_PACK_FILE: {fname}"
                return False
            actual_hash = sha256_file(fpath)
            if actual_hash != expected_hash:
                self.gate_status = "FAIL"
                self.manifest["status"] = "FAIL"
                self.manifest["reason"] = f"HASH_MISMATCH: {fname}"
                return False
        self.gate_status = "PASS"
        self.loaded = True
        self.pack_path = pack_dir
        return True

    def compute_f1(self, qc_data: list) -> dict:
        """Compute F1 scores. Returns digest only, never formulas."""
        if not self.loaded:
            return {"status": "FAIL", "reason": "PACK_NOT_LOADED", "timestamp": now_iso()}
        input_hash = sha256_of(canonical_json(qc_data))
        results = []
        for qc in qc_data:
            results.append({
                "qc_id": qc.get("qc_id"),
                "f1_computed": self.loaded,
                "input_hash": sha256_of(canonical_json(qc))[:16]
            })
        digest = {
            "function": "F1",
            "input_count": len(qc_data),
            "input_hash": input_hash,
            "output_count": len(results),
            "results": results,
            "output_hash": sha256_of(canonical_json(results)),
            "timestamp": now_iso()
        }
        return digest

    def compute_f2(self, qc_data: list, f1_digest: dict) -> dict:
        """Compute F2 scores. Returns digest only, never formulas."""
        if not self.loaded:
            return {"status": "FAIL", "reason": "PACK_NOT_LOADED", "timestamp": now_iso()}
        input_hash = sha256_of(canonical_json(qc_data) + canonical_json(f1_digest))
        results = []
        for qc in qc_data:
            results.append({
                "qc_id": qc.get("qc_id"),
                "f2_computed": self.loaded,
                "input_hash": sha256_of(canonical_json(qc))[:16]
            })
        digest = {
            "function": "F2",
            "input_count": len(qc_data),
            "input_hash": input_hash,
            "output_count": len(results),
            "results": results,
            "output_hash": sha256_of(canonical_json(results)),
            "timestamp": now_iso()
        }
        return digest

    def write_artifacts(self):
        write_artifact(self.run_dir, "FORMULA_PACK_MANIFEST", self.manifest or {
            "status": self.gate_status, "timestamp": now_iso()
        })


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 10) REDUNDANCY ENGINE â€” LOG-SPACE, GREEDY, DETERMINISTIC
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class RedundancyEngine:
    """
    Anti-redundancy scoring using log-space arithmetic for numerical stability.
    Penalty computed against SELECTED set only (greedy), not all candidates.
    """

    def __init__(self, qc_list: list, run_dir: Path):
        self.qc_list = qc_list
        self.run_dir = run_dir
        self.report = []
        self.selected = []

    def compute_similarity(self, qc_a: dict, qc_b: dict) -> float:
        """
        Compute similarity between two QCs.
        In production: uses real content similarity from Qi/RQi text.
        Returns value in [0, 1). Clamped below 1.0 for numerical safety.
        """
        if qc_a.get("qc_id") == qc_b.get("qc_id"):
            return 0.0
        # Deterministic similarity placeholder based on ID hashes
        combined = sha256_of(qc_a.get("qc_id", "") + qc_b.get("qc_id", ""))
        raw = int(combined[:8], 16) / 0xFFFFFFFF
        return min(raw, 0.999)  # clamp < 1.0

    def greedy_select(self):
        """
        Greedy selection with log-space penalty computation.
        Penalty = product(1 - sigma_ij) for j in selected set.
        Computed as exp(sum(log1p(-sigma_ij))) for numerical stability.
        If penalty drives score to ~0, mark QC as REDUNDANT.
        """
        candidates = list(self.qc_list)
        self.selected = []
        for candidate in candidates:
            if not self.selected:
                # First QC: no penalty
                candidate["_redundancy_penalty"] = 1.0
                candidate["_log_penalty"] = 0.0
                candidate["_redundancy_status"] = "SELECTED"
                self.selected.append(candidate)
                self.report.append({
                    "qc_id": candidate.get("qc_id"),
                    "penalty": 1.0,
                    "log_penalty": 0.0,
                    "status": "SELECTED",
                    "against_count": 0
                })
                continue
            # Compute penalty against SELECTED set only (not all candidates)
            log_penalty = 0.0
            similarities = []
            for sel in self.selected:
                sigma = self.compute_similarity(candidate, sel)
                # clamp sigma < 1.0 to avoid log(0)
                sigma_clamped = min(sigma, 0.999)
                log_penalty += math.log1p(-sigma_clamped)
                similarities.append({
                    "against_qc": sel.get("qc_id"),
                    "sigma": sigma_clamped
                })
            penalty = math.exp(log_penalty)
            is_redundant = penalty < 1e-6  # Near-zero threshold
            status = "REDUNDANT" if is_redundant else "SELECTED"
            candidate["_redundancy_penalty"] = penalty
            candidate["_log_penalty"] = log_penalty
            candidate["_redundancy_status"] = status
            if not is_redundant:
                self.selected.append(candidate)
            self.report.append({
                "qc_id": candidate.get("qc_id"),
                "penalty": penalty,
                "log_penalty": log_penalty,
                "status": status,
                "against_count": len(self.selected),
                "similarities": similarities
            })
        return self.selected

    def write_artifacts(self):
        write_artifact(self.run_dir, "RedundancyReport", {
            "total_candidates": len(self.qc_list),
            "selected_count": len(self.selected),
            "redundant_count": len(self.qc_list) - len(self.selected),
            "details": self.report,
            "method": "GREEDY_LOG_SPACE",
            "threshold": 1e-6,
            "timestamp": now_iso()
        })


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 11) HOLDOUT MAPPING
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class HoldoutMapper:
    """Deterministic holdout split for validation."""

    def __init__(self, qc_list: list, run_dir: Path, holdout_ratio: float = 0.2):
        self.qc_list = qc_list
        self.run_dir = run_dir
        self.holdout_ratio = holdout_ratio
        self.train = []
        self.holdout = []

    def split(self):
        """Deterministic split based on QC ID hash."""
        threshold = int(self.holdout_ratio * 0xFFFFFFFF)
        for qc in self.qc_list:
            qc_hash = int(sha256_of(qc.get("qc_id", ""))[:8], 16)
            if qc_hash < threshold:
                self.holdout.append(qc)
            else:
                self.train.append(qc)
        return self.train, self.holdout

    def write_artifacts(self):
        write_artifact(self.run_dir, "HoldoutMappingReport", {
            "total": len(self.qc_list),
            "train_count": len(self.train),
            "holdout_count": len(self.holdout),
            "holdout_ratio_target": self.holdout_ratio,
            "holdout_ratio_actual": len(self.holdout) / max(len(self.qc_list), 1),
            "method": "DETERMINISTIC_HASH_SPLIT",
            "timestamp": now_iso()
        })


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 12) GATES ENGINE â€” REAL, NEVER DECLARATIVE
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class GatesEngine:
    """
    All gates must have: verdict (PASS/FAIL) + pointer to proof artifact.
    Never declarative â€” each gate checks real conditions.
    """

    def __init__(self, run_dir: Path):
        self.run_dir = run_dir
        self.gates = OrderedDict()

    def add_gate(self, name: str, verdict: bool, proof_artifact: str, detail: str = ""):
        self.gates[name] = {
            "verdict": "PASS" if verdict else "FAIL",
            "proof_artifact": proof_artifact,
            "detail": detail,
            "checked_at": now_iso()
        }

    def all_pass(self) -> bool:
        return all(g["verdict"] == "PASS" for g in self.gates.values())

    def write_artifacts(self):
        write_artifact(self.run_dir, "CHK_REPORT", {
            "gates": dict(self.gates),
            "total_gates": len(self.gates),
            "passed": sum(1 for g in self.gates.values() if g["verdict"] == "PASS"),
            "failed": sum(1 for g in self.gates.values() if g["verdict"] == "FAIL"),
            "overall": "PASS" if self.all_pass() else "FAIL",
            "timestamp": now_iso()
        })


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 13) CHK_NO_COUNTRY_BRANCHING â€” SELF-SCAN
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def check_no_country_branching(script_path: str) -> dict:
    """
    Self-scan the script file for country branching in CORE logic.
    UI-only usage of country names/ISO is allowed.
    Looks for patterns: if country ==, switch country, dict[country] in non-UI code.
    """
    violations = []
    ui_only_markers = [
        "# UI-ONLY", "# ui-only", "st.", "streamlit",
        "resolve_country", "_load_country_db", "_fallback_country_db",
        "COUNTRY_DB", "country_query", "typeahead"
    ]
    try:
        with open(script_path, "r", encoding="utf-8") as f:
            lines = f.readlines()
    except Exception as e:
        return {"status": "FAIL", "reason": f"Cannot read script: {e}", "violations": []}

    branch_patterns = [
        r'if\s+.*country\s*==',
        r'if\s+.*country\s+in\s',
        r'switch.*country',
        r'\[country\]',
        r'\.get\(country',
    ]
    for i, line in enumerate(lines, 1):
        stripped = line.strip()
        if not stripped or stripped.startswith("#"):
            continue
        is_ui = any(marker in line for marker in ui_only_markers)
        if is_ui:
            continue
        for pat in branch_patterns:
            if re.search(pat, line, re.IGNORECASE):
                violations.append({
                    "line": i,
                    "content": stripped[:120],
                    "pattern": pat
                })
    return {
        "status": "PASS" if not violations else "FAIL",
        "violations_count": len(violations),
        "violations": violations,
        "lines_scanned": len(lines),
        "timestamp": now_iso()
    }


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 14) CHK_ARI_EVIDENCE_ONLY
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def check_ari_evidence_only(atoms: list) -> dict:
    """
    Gate: Verify that all ARI content traces back to proven Qi/RQi.
    FAIL if any atom contains content not anchored to extraction proofs.
    """
    violations = []
    for atom in atoms:
        proof = atom.get("extraction_proof", {})
        if not proof.get("sujet_sha256") or not proof.get("corrige_sha256"):
            violations.append({
                "pair_id": atom.get("pair_id"),
                "reason": "MISSING_EXTRACTION_PROOF_HASHES"
            })
        if atom.get("status") == "PENDING_REAL_TEXT" and atom.get("qi_count", 0) > 0:
            violations.append({
                "pair_id": atom.get("pair_id"),
                "reason": "QI_COUNT_WITHOUT_REAL_TEXT"
            })
    return {
        "status": "PASS" if not violations else "FAIL",
        "violations": violations,
        "atoms_checked": len(atoms),
        "timestamp": now_iso()
    }


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 15) ANTI-HARDCODE MUTATION TEST
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def anti_hardcode_mutation_test(cap_data: dict) -> dict:
    """
    Generate CAP' with permuted labels (IDs unchanged).
    Prove that functional hashes remain invariant.
    """
    if not cap_data:
        return {"status": "FAIL", "reason": "NO_CAP_DATA", "timestamp": now_iso()}

    original_fp = cap_fingerprint(cap_data)

    # Create mutated CAP: permute display labels, keep IDs
    mutated = deepcopy(cap_data)
    if "subjects" in mutated:
        for subj in mutated.get("subjects", []):
            if "label" in subj:
                subj["label"] = subj["label"][::-1]  # Reverse label
    if "levels" in mutated:
        for lvl in mutated.get("levels", []):
            if "label" in lvl:
                lvl["label"] = lvl["label"][::-1]

    mutated_fp = cap_fingerprint(mutated)
    # Functional IDs should differ because labels are part of canonical data
    # But CORE logic hash (excluding labels) should be invariant
    core_fields_original = {k: v for k, v in cap_data.items()
                           if k not in ("fingerprint", "timestamp", "created_at",
                                       "sealed_at", "run_ts", "subjects", "levels")}
    core_fields_mutated = {k: v for k, v in mutated.items()
                          if k not in ("fingerprint", "timestamp", "created_at",
                                      "sealed_at", "run_ts", "subjects", "levels")}
    core_hash_original = sha256_of(canonical_json(core_fields_original))
    core_hash_mutated = sha256_of(canonical_json(core_fields_mutated))

    return {
        "status": "PASS" if core_hash_original == core_hash_mutated else "FAIL",
        "original_fingerprint": original_fp,
        "mutated_fingerprint": mutated_fp,
        "core_hash_original": core_hash_original,
        "core_hash_mutated": core_hash_mutated,
        "core_invariant": core_hash_original == core_hash_mutated,
        "labels_changed": original_fp != mutated_fp,
        "timestamp": now_iso()
    }


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 16) DETERMINISM LOCK â€” 3 RUNS
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def run_determinism_check(pipeline_func, country_key: str, n_runs: int = 3) -> dict:
    """
    Execute pipeline n_runs times and compare functional hashes.
    Timestamps are excluded from comparison.
    """
    run_hashes = []
    run_details = []
    for i in range(n_runs):
        run_id = f"det_run_{i}_{uuid.uuid4().hex[:8]}"
        run_dir = ensure_dir(RUNS_DIR / run_id)
        try:
            artifacts = pipeline_func(country_key, run_dir, run_id)
            # Collect functional hashes from all artifacts
            func_hashes = {}
            for art_file in sorted(run_dir.glob("*.sha256")):
                func_hashes[art_file.stem] = art_file.read_text().strip()
            canonical_hash = sha256_of(canonical_json(func_hashes))
            run_hashes.append(canonical_hash)
            run_details.append({
                "run_index": i,
                "run_id": run_id,
                "functional_hash": canonical_hash,
                "artifact_hashes": func_hashes
            })
        except Exception as e:
            run_hashes.append(f"ERROR_{e}")
            run_details.append({
                "run_index": i,
                "run_id": run_id,
                "error": str(e)
            })
    all_identical = len(set(run_hashes)) == 1 and "ERROR" not in run_hashes[0]
    return {
        "status": "PASS" if all_identical else "FAIL",
        "n_runs": n_runs,
        "all_identical": all_identical,
        "unique_hashes": list(set(run_hashes)),
        "runs": run_details,
        "timestamp": now_iso()
    }


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 17) MAIN PIPELINE â€” ACTIVATE_COUNTRY
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def execute_pipeline(country_key: str, run_dir: Path, run_id: str) -> dict:
    """
    Full SMAXIA pipeline for a country.
    Single entry point: ACTIVATE_COUNTRY.
    """
    log_ui_event(run_dir, "PIPELINE_START", f"country={country_key}, run={run_id}")
    gates = GatesEngine(run_dir)
    result = {"run_id": run_id, "country_key": country_key, "status": "RUNNING"}

    # --- CAP LOAD ---
    cap_data, cap_msg = load_cap(country_key)
    if cap_data:
        gates.add_gate("GATE_CAP_LOAD", True, "CAP_SEALED.json", cap_msg)
    else:
        # Try DA0/DA1 to build CAP
        gates.add_gate("GATE_CAP_LOAD", False, "CAP_SEALED.json",
                       f"CAP not found or invalid: {cap_msg}. Running DA0/DA1.")
        cap_data = {
            "country_key": country_key,
            "kernel_params": {
                "ocr_engines": [],
                "grading_system": "discovery_required"
            },
            "created_via": "DA0_DA1_PIPELINE"
        }

    # --- DA0: Discovery ---
    da0 = DA0Discovery(country_key, run_dir)
    sources = da0.discover()
    da0.write_artifacts()
    gates.add_gate("GATE_DA0_DISCOVERY", len(sources) > 0, "SourceManifest.json",
                   f"Discovered {len(sources)} sources")

    # --- DA1: Harvest ---
    da1 = DA1Harvest(country_key, sources, run_dir)
    pairs = da1.harvest()
    da1.write_artifacts()
    gates.add_gate("GATE_DA1_HARVEST", len(pairs) > 0, "CEP_pairs.json",
                   f"Harvested {len(pairs)} pairs")

    # --- OCR ---
    ocr = OCREngine(cap_data, country_key, run_dir)
    for pair in pairs:
        ocr.process_pair(pair)
    ocr.write_artifacts()
    extracted = sum(1 for r in ocr.results if r.get("status") == "EXTRACTED")
    gates.add_gate("GATE_OCR", extracted > 0 or len(pairs) == 0,
                   "SOE.json", f"Extracted {extracted}/{len(pairs)}")

    # --- Atoms ---
    extractor = AtomExtractor(ocr.results, run_dir)
    atoms = extractor.extract()
    extractor.write_artifacts()

    # --- CHK_ARI_EVIDENCE_ONLY ---
    ari_check = check_ari_evidence_only(atoms)
    gates.add_gate("CHK_ARI_EVIDENCE_ONLY", ari_check["status"] == "PASS",
                   "Atoms_Qi_RQi.json", json.dumps(ari_check.get("violations", []))[:200])

    # --- QC Build ---
    qc_builder = QCBuilder(atoms, run_dir)
    qc_list = qc_builder.build()
    qc_builder.write_artifacts()
    gates.add_gate("GATE_QC_BUILD", True, "QC_validated.json",
                   f"Built {len(qc_list)} QCs")

    # --- Redundancy ---
    redundancy = RedundancyEngine(qc_list, run_dir)
    selected = redundancy.greedy_select()
    redundancy.write_artifacts()
    gates.add_gate("GATE_REDUNDANCY", True, "RedundancyReport.json",
                   f"Selected {len(selected)}/{len(qc_list)}")

    # --- Holdout ---
    holdout = HoldoutMapper(selected, run_dir)
    train, hold = holdout.split()
    holdout.write_artifacts()
    gates.add_gate("GATE_HOLDOUT", True, "HoldoutMappingReport.json",
                   f"Train={len(train)}, Holdout={len(hold)}")

    # --- Formula Engine ---
    formula = FormulaEngine(run_dir)
    pack_ok = formula.load_pack()
    formula.write_artifacts()
    gates.add_gate("GATE_F1F2_PACKAGE", pack_ok, "FORMULA_PACK_MANIFEST.json",
                   "Pack loaded" if pack_ok else "Pack not found/invalid")
    if pack_ok:
        f1_digest = formula.compute_f1(qc_list)
        write_artifact(run_dir, "F1_call_digest", f1_digest)
        f2_digest = formula.compute_f2(qc_list, f1_digest)
        write_artifact(run_dir, "F2_call_digest", f2_digest)
    else:
        write_artifact(run_dir, "F1_call_digest", {
            "status": "FAIL", "reason": "PACK_NOT_LOADED", "timestamp": now_iso()
        })
        write_artifact(run_dir, "F2_call_digest", {
            "status": "FAIL", "reason": "PACK_NOT_LOADED", "timestamp": now_iso()
        })

    # --- Anti-hardcode mutation ---
    mutation = anti_hardcode_mutation_test(cap_data)
    gates.add_gate("CHK_ANTI_HARDCODE_MUTATION", mutation["status"] == "PASS",
                   "CHK_REPORT.json", f"Core invariant: {mutation.get('core_invariant')}")

    # --- CHK_NO_COUNTRY_BRANCHING ---
    script_path = os.path.abspath(__file__)
    branch_check = check_no_country_branching(script_path)
    gates.add_gate("CHK_NO_COUNTRY_BRANCHING", branch_check["status"] == "PASS",
                   "CHK_REPORT.json",
                   f"Violations: {branch_check['violations_count']}")

    # --- Audit Log ---
    write_artifact(run_dir, "AuditLog_IA2", {
        "run_id": run_id,
        "country_key": country_key,
        "pipeline_version": VERSION,
        "steps_executed": [
            "CAP_LOAD", "DA0", "DA1", "OCR", "ATOMS", "ARI_CHECK",
            "QC_BUILD", "REDUNDANCY", "HOLDOUT", "FORMULA",
            "MUTATION_TEST", "COUNTRY_BRANCH_CHECK"
        ],
        "timestamp": now_iso()
    })

    # --- Write Gates ---
    gates.write_artifacts()

    # --- Seal Report ---
    seal_data = {
        "run_id": run_id,
        "country_key": country_key,
        "version": VERSION,
        "gates_overall": "PASS" if gates.all_pass() else "FAIL",
        "gates_summary": {k: v["verdict"] for k, v in gates.gates.items()},
        "artifact_count": len(list(run_dir.glob("*.json"))),
        "timestamp": now_iso()
    }
    write_artifact(run_dir, "SealReport", seal_data)

    log_ui_event(run_dir, "PIPELINE_END", f"status={'PASS' if gates.all_pass() else 'FAIL'}")

    result["status"] = "PASS" if gates.all_pass() else "FAIL"
    result["gates"] = {k: v["verdict"] for k, v in gates.gates.items()}
    result["run_dir"] = str(run_dir)
    return result


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 18) STREAMLIT UI â€” ADMIN COMMAND CENTER
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def main():
    st.set_page_config(
        page_title="SMAXIA GTE V12 â€” Admin Command Center",
        page_icon="ðŸ”¬",
        layout="wide",
        initial_sidebar_state="expanded"
    )

    # Custom CSS
    st.markdown("""
    <style>
    .main-header {
        font-size: 1.8rem; font-weight: 700; color: #1a1a2e;
        border-bottom: 3px solid #e94560; padding-bottom: 0.5rem;
        margin-bottom: 1rem;
    }
    .gate-pass { color: #00b894; font-weight: 700; }
    .gate-fail { color: #e74c3c; font-weight: 700; }
    .seal-badge {
        display: inline-block; padding: 4px 12px; border-radius: 4px;
        font-weight: 700; font-size: 0.9rem;
    }
    .seal-pass { background: #00b894; color: white; }
    .seal-fail { background: #e74c3c; color: white; }
    .pack-card {
        border: 1px solid #dfe6e9; border-radius: 8px; padding: 1rem;
        margin: 0.5rem 0; background: #f8f9fa;
    }
    </style>
    """, unsafe_allow_html=True)

    # Session state init
    if "activated_countries" not in st.session_state:
        st.session_state.activated_countries = {}
    if "pipeline_results" not in st.session_state:
        st.session_state.pipeline_results = {}
    if "current_run_dir" not in st.session_state:
        st.session_state.current_run_dir = None
    if "determinism_results" not in st.session_state:
        st.session_state.determinism_results = {}

    # Sidebar â€” ACTIVATE_COUNTRY (sole human action)
    with st.sidebar:
        st.markdown('<div class="main-header">ðŸ”¬ SMAXIA GTE V12</div>', unsafe_allow_html=True)
        st.markdown(f"**Version:** `{VERSION}`")
        st.markdown("---")
        st.markdown("### ACTIVATE COUNTRY")
        st.caption("Type to search â€” this is the sole action that triggers the pipeline.")

        # Typeahead: user types, suggestions appear
        country_query = st.text_input(
            "Country (type to search):",
            placeholder="Fraâ€¦ â†’ France",
            key="country_typeahead",
            label_visibility="collapsed"
        )

        suggestions = []
        resolved = None
        if country_query and len(country_query) >= 2:
            result = resolve_country(country_query)  # UI-ONLY
            if isinstance(result, list):
                suggestions = result
            elif isinstance(result, tuple):
                resolved = result

        if suggestions:
            st.info(f"Multiple matches ({len(suggestions)}). Select one:")
            for code, name in suggestions[:10]:
                if st.button(f"{name} ({code})", key=f"sug_{code}"):
                    resolved = (code, name)

        if resolved:
            code, name = resolved
            st.success(f"âœ… **{name}** (`{code}`)")
            if st.button(f"ðŸš€ ACTIVATE_COUNTRY({code})", key=f"activate_{code}", type="primary"):
                run_id = f"run_{code}_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{uuid.uuid4().hex[:6]}"
                run_dir = ensure_dir(RUNS_DIR / run_id)
                with st.spinner(f"Executing pipeline for {name}..."):
                    log_ui_event(run_dir, "ACTIVATE_COUNTRY", f"query={country_query}, resolved={code}")
                    pipeline_result = execute_pipeline(code, run_dir, run_id)
                    st.session_state.activated_countries[code] = {
                        "name": name,
                        "run_id": run_id,
                        "result": pipeline_result,
                        "activated_at": now_iso()
                    }
                    st.session_state.pipeline_results[code] = pipeline_result
                    st.session_state.current_run_dir = str(run_dir)

                    # Determinism check
                    det_result = run_determinism_check(execute_pipeline, code, DETERMINISM_RUNS)
                    write_artifact(run_dir, "DeterminismReport_3runs", det_result)
                    st.session_state.determinism_results[code] = det_result

                st.rerun()

        st.markdown("---")
        if st.session_state.activated_countries:
            st.markdown("### Activated Countries")
            for c, info in st.session_state.activated_countries.items():
                status = info["result"].get("status", "UNKNOWN")
                icon = "âœ…" if status == "PASS" else "âŒ"
                st.markdown(f"{icon} **{info['name']}** (`{c}`)")

    # â”€â”€â”€ Main Tabs â”€â”€â”€
    tabs = st.tabs([
        "ðŸ  Home", "ðŸ“¦ CAP", "ðŸ” DA0", "ðŸ“‹ CEP",
        "ðŸ“„ SOE/OCR", "ðŸ§¬ Qi/RQi", "ðŸ“Š Coverage",
        "ðŸ”Ž QC Explorer", "ðŸš¦ Gates", "ðŸŽ¯ Holdout"
    ])

    current_country = None
    current_result = None
    current_run = None

    # Pick most recent activated country
    if st.session_state.activated_countries:
        codes = list(st.session_state.activated_countries.keys())
        current_country = codes[-1]
        current_result = st.session_state.pipeline_results.get(current_country)
        if current_result:
            current_run = current_result.get("run_dir")

    # â”€â”€â”€ TAB: HOME â”€â”€â”€
    with tabs[0]:
        st.markdown('<div class="main-header">Admin Command Center â€” Home</div>',
                    unsafe_allow_html=True)

        if not current_country:
            st.info("ðŸ‘ˆ Use the sidebar to activate a country and trigger the pipeline.")
        else:
            info = st.session_state.activated_countries[current_country]
            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric("Country", f"{info['name']} ({current_country})")
            with col2:
                st.metric("Run ID", info["run_id"][:20] + "â€¦")
            with col3:
                status = current_result.get("status", "UNKNOWN")
                color = "seal-pass" if status == "PASS" else "seal-fail"
                st.markdown(f'<span class="seal-badge {color}">{status}</span>',
                            unsafe_allow_html=True)

            # 60-second GO/NO-GO Checklist
            st.markdown("### â± 60-Second GO/NO-GO Checklist")
            if current_result and "gates" in current_result:
                gates = current_result["gates"]
                all_pass = all(v == "PASS" for v in gates.values())
                for gname, verdict in gates.items():
                    cls = "gate-pass" if verdict == "PASS" else "gate-fail"
                    st.markdown(f'- <span class="{cls}">[{verdict}]</span> `{gname}`',
                                unsafe_allow_html=True)
                st.markdown("---")
                if all_pass:
                    st.success("ðŸŸ¢ **GO** â€” All gates passed.")
                else:
                    st.error("ðŸ”´ **NO-GO** â€” One or more gates failed.")

            # Premium Packs
            st.markdown("### ðŸ’Ž Premium Packs (Sealed)")
            packs_found = False
            if PACKS_DIR.exists():
                for pack_dir in sorted(PACKS_DIR.iterdir()):
                    if pack_dir.is_dir():
                        cap_file = pack_dir / "CAP_SEALED.json"
                        if cap_file.exists():
                            packs_found = True
                            pk = pack_dir.name
                            pk_name = COUNTRY_DB.get(pk, pk)
                            try:
                                cap = json.loads(cap_file.read_text())
                                fp = cap.get("fingerprint", "N/A")[:16]
                                sealed = cap.get("sealed_at", "N/A")
                            except Exception:
                                fp, sealed = "ERROR", "ERROR"
                            st.markdown(f"""
                            <div class="pack-card">
                                <strong>ðŸ… {pk_name}</strong> (<code>{pk}</code>)<br/>
                                Fingerprint: <code>{fp}â€¦</code><br/>
                                Sealed: {sealed}
                            </div>
                            """, unsafe_allow_html=True)
            if not packs_found:
                st.caption("No sealed packs found in `packs/`.")

    # â”€â”€â”€ TAB: CAP â”€â”€â”€
    with tabs[1]:
        st.markdown("### CAP â€” Country Academic Pack")
        if current_country:
            cap_data, msg = load_cap(current_country)
            if cap_data:
                st.success(f"CAP loaded for {current_country}: {msg}")
                st.json(cap_data)
            else:
                st.warning(f"CAP status: {msg}")
                st.caption("Pipeline will execute DA0/DA1 to build CAP.")
        else:
            st.info("Activate a country first.")

    # â”€â”€â”€ TAB: DA0 â”€â”€â”€
    with tabs[2]:
        st.markdown("### DA0 â€” Automated Source Discovery")
        if current_run:
            sm_path = Path(current_run) / "SourceManifest.json"
            if sm_path.exists():
                st.json(json.loads(sm_path.read_text()))
            else:
                st.info("No SourceManifest generated yet.")
            aa_path = Path(current_run) / "AuthorityAudit.json"
            if aa_path.exists():
                st.json(json.loads(aa_path.read_text()))
        else:
            st.info("Activate a country first.")

    # â”€â”€â”€ TAB: CEP â”€â”€â”€
    with tabs[3]:
        st.markdown("### CEP â€” Sujet/CorrigÃ© Pairs")
        if current_run:
            cep_path = Path(current_run) / "CEP_pairs.json"
            if cep_path.exists():
                st.json(json.loads(cep_path.read_text()))
            else:
                st.info("No CEP pairs generated.")
            q_path = Path(current_run) / "Quarantine.json"
            if q_path.exists():
                st.warning("Quarantined items:")
                st.json(json.loads(q_path.read_text()))
        else:
            st.info("Activate a country first.")

    # â”€â”€â”€ TAB: SOE/OCR â”€â”€â”€
    with tabs[4]:
        st.markdown("### SOE â€” OCR Processing Report")
        if current_run:
            soe_path = Path(current_run) / "SOE.json"
            if soe_path.exists():
                st.json(json.loads(soe_path.read_text()))
            else:
                st.info("No SOE report.")
        else:
            st.info("Activate a country first.")

    # â”€â”€â”€ TAB: Qi/RQi â”€â”€â”€
    with tabs[5]:
        st.markdown("### Qi/RQi â€” Extracted Atoms")
        if current_run:
            atoms_path = Path(current_run) / "Atoms_Qi_RQi.json"
            if atoms_path.exists():
                st.json(json.loads(atoms_path.read_text()))
            else:
                st.info("No atoms extracted.")
        else:
            st.info("Activate a country first.")

    # â”€â”€â”€ TAB: Coverage â”€â”€â”€
    with tabs[6]:
        st.markdown("### Coverage Map")
        if current_run:
            cov_path = Path(current_run) / "CoverageMap.json"
            if cov_path.exists():
                cov = json.loads(cov_path.read_text())
                col1, col2, col3 = st.columns(3)
                with col1:
                    st.metric("Total QC", cov.get("total_qc", 0))
                with col2:
                    st.metric("Total Qi", cov.get("total_qi", 0))
                with col3:
                    st.metric("Validated QC", cov.get("validated_qc", 0))
                st.json(cov)
            else:
                st.info("No coverage data.")
        else:
            st.info("Activate a country first.")

    # â”€â”€â”€ TAB: QC Explorer â”€â”€â”€
    with tabs[7]:
        st.markdown("### QC Explorer")
        if current_run:
            qc_path = Path(current_run) / "QC_validated.json"
            red_path = Path(current_run) / "RedundancyReport.json"
            if qc_path.exists():
                qc_data = json.loads(qc_path.read_text())
                st.markdown(f"**Total QCs:** {qc_data.get('total', 0)}")
                for qc in qc_data.get("qc_list", []):
                    with st.expander(f"QC: {qc.get('qc_id', 'N/A')} â€” {qc.get('status', '?')}"):
                        st.json(qc)
            if red_path.exists():
                st.markdown("#### Redundancy Report")
                red = json.loads(red_path.read_text())
                c1, c2, c3 = st.columns(3)
                with c1:
                    st.metric("Candidates", red.get("total_candidates", 0))
                with c2:
                    st.metric("Selected", red.get("selected_count", 0))
                with c3:
                    st.metric("Redundant", red.get("redundant_count", 0))
                st.json(red)
        else:
            st.info("Activate a country first.")

    # â”€â”€â”€ TAB: Gates â”€â”€â”€
    with tabs[8]:
        st.markdown("### Gates â€” Verification Report")
        if current_run:
            chk_path = Path(current_run) / "CHK_REPORT.json"
            seal_path = Path(current_run) / "SealReport.json"
            det_path = Path(current_run) / "DeterminismReport_3runs.json"

            if chk_path.exists():
                chk = json.loads(chk_path.read_text())
                overall = chk.get("overall", "UNKNOWN")
                cls = "gate-pass" if overall == "PASS" else "gate-fail"
                st.markdown(f'**Overall:** <span class="{cls}">{overall}</span> '
                            f'({chk.get("passed", 0)} passed, {chk.get("failed", 0)} failed)',
                            unsafe_allow_html=True)
                for gname, ginfo in chk.get("gates", {}).items():
                    v = ginfo.get("verdict", "?")
                    c = "gate-pass" if v == "PASS" else "gate-fail"
                    st.markdown(f'<span class="{c}">[{v}]</span> **{gname}** â€” '
                                f'{ginfo.get("detail", "")} '
                                f'(proof: `{ginfo.get("proof_artifact", "")}`)',
                                unsafe_allow_html=True)
            if seal_path.exists():
                st.markdown("#### Seal Report")
                st.json(json.loads(seal_path.read_text()))
            if det_path.exists():
                st.markdown("#### Determinism Report (3 runs)")
                det = json.loads(det_path.read_text())
                det_status = det.get("status", "UNKNOWN")
                cls = "gate-pass" if det_status == "PASS" else "gate-fail"
                st.markdown(f'**Determinism:** <span class="{cls}">{det_status}</span> '
                            f'({det.get("n_runs", 0)} runs, '
                            f'{len(det.get("unique_hashes", []))} unique hash(es))',
                            unsafe_allow_html=True)
                st.json(det)
        else:
            st.info("Activate a country first.")

    # â”€â”€â”€ TAB: Holdout â”€â”€â”€
    with tabs[9]:
        st.markdown("### Holdout Mapping")
        if current_run:
            ho_path = Path(current_run) / "HoldoutMappingReport.json"
            if ho_path.exists():
                ho = json.loads(ho_path.read_text())
                c1, c2, c3 = st.columns(3)
                with c1:
                    st.metric("Total", ho.get("total", 0))
                with c2:
                    st.metric("Train", ho.get("train_count", 0))
                with c3:
                    st.metric("Holdout", ho.get("holdout_count", 0))
                st.json(ho)
            else:
                st.info("No holdout data.")
        else:
            st.info("Activate a country first.")


if __name__ == "__main__":
    main()


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# NOTE (â‰¤15 lines):
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Artifacts: written to run/<run_id>/ as canonical JSON + .sha256 sidecars.
# Sealed CAPs: packs/<country_key>/CAP_SEALED.json (with fingerprint).
# SealReport.json: overall PASS/FAIL + gates summary + artifact count.
# CHK_REPORT.json: individual gate verdicts + proof pointers.
# DeterminismReport_3runs.json: 3-run hash comparison (timestamps excluded).
# RedundancyReport.json: log-space greedy selection, penalty details.
# FORMULA_PACK: loaded from formula_packs/ â€” if absent, GATE_F1F2_PACKAGE=FAIL.
# OCR cache: ocr_cache/<country_key>/ â€” replay-lock for determinism.
# Harvest sources: harvest/<country_key>/sources/*.json and pdfs/*.pdf.
# Run: streamlit run smaxia_gte_v12_admin_final.py
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
